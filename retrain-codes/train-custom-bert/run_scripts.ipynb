{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parts of the sample container\n",
    "\n",
    "In the `container` directory are all the components you need to package the sample algorithm for Amazon SageMager:\n",
    "\n",
    "    .\n",
    "    |-- Dockerfile\n",
    "    |-- build_and_push.sh\n",
    "    `-- codes\n",
    "        |-- train\n",
    "        `\n",
    "\n",
    "Let's discuss each of these in turn:\n",
    "\n",
    "* __`Dockerfile`__ describes how to build your Docker container image. More details below.\n",
    "* __`build_and_push.sh`__ is a script that uses the Dockerfile to build your container images and then pushes it to ECR. We'll invoke the commands directly later in this notebook, but you can just copy and run the script for your own algorithms.\n",
    "* __`codes`__ is the directory which contains the files that will be installed in the container.\n",
    "\n",
    "The files that we'll put in the `codes` container are:\n",
    "\n",
    "* __`train`__ is the program that is invoked when the container is run for training. You will modify this program to implement your training algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Build an image that can do training and inference in SageMaker\n",
      "# This is a Python 3 image that uses the nginx, gunicorn, flask stack\n",
      "# for serving inferences in a stable way.\n",
      "\n",
      "FROM ubuntu:20.04\n",
      "\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
      "         wget \\\n",
      "         python3-pip \\\n",
      "         python3.8 \\\n",
      "         python3-setuptools \\\n",
      "         nginx \\\n",
      "         ca-certificates \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      "\n",
      "# Here we get all python packages.\n",
      "# There's substantial overlap between scipy and numpy that we eliminate by\n",
      "# linking them together. Likewise, pip leaves the install caches populated which uses\n",
      "# a significant amount of space. These optimizations save a fair amount of space in the\n",
      "# image, which reduces start up time.\n",
      "RUN pip3 --no-cache-dir install numpy==1.22 scipy scikit-learn pandas nltk joblib torch torchvision tensorflow sentence_transformers\n",
      "\n",
      "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
      "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
      "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
      "# PATH so that the train and serve programs are found when the container is invoked.\n",
      "\n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Set up the program in the image\n",
      "COPY codes /opt/program\n",
      "WORKDIR /opt/program\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to ECR using `docker push`. This code is also available as the shell script `container/build-and-push.sh`, which you can run as `build-and-push.sh custom-sentence-transformer` to build the image `custom-sentence-transformer`. \n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  22.02kB\n",
      "Step 1/9 : FROM ubuntu:20.04\n",
      " ---> 1c5c8d0b973a\n",
      "Step 2/9 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3-pip          python3.8          python3-setuptools          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 13321fc815ad\n",
      "Step 3/9 : RUN ln -s /usr/bin/python3 /usr/bin/python\n",
      " ---> Using cache\n",
      " ---> 1948e45e9d27\n",
      "Step 4/9 : RUN pip3 --no-cache-dir install numpy==1.22 scipy scikit-learn pandas nltk joblib torch torchvision tensorflow sentence_transformers\n",
      " ---> Running in 5111adce08df\n",
      "Collecting numpy==1.22\n",
      "  Downloading numpy-1.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.1-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.8.tar.gz (1.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (0.34.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
      "Collecting ml-dtypes>=0.0.3\n",
      "  Downloading ml_dtypes-0.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.1-py2.py3-none-any.whl (178 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-6.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: sentence-transformers, jax, lit\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125922 sha256=4703b9f468ffe8135396dad254429325a60a31848d7d0bce7cd173a647e5d852\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8vacx6qu/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "  Building wheel for jax (PEP 517): started\n",
      "  Building wheel for jax (PEP 517): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439678 sha256=a4033cb6ea6da8ae7d8ffeb76788e663e99866914ff27603d168e396218ef623\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8vacx6qu/wheels/45/83/1e/3db22c5e1941c10e41c4f5cdf829b0a358146d4d0733d4a105\n",
      "  Building wheel for lit (setup.py): started\n",
      "  Building wheel for lit (setup.py): finished with status 'done'\n",
      "  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93585 sha256=9d649921c4a12592a33b496ca5ca0b34fa3f6043f36ed02f1443a4102799cac5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8vacx6qu/wheels/30/c8/c0/9d24f63f30f4879b8efb718a11b2191ef5c8510259d1d6c392\n",
      "Successfully built sentence-transformers jax lit\n",
      "Installing collected packages: numpy, scipy, joblib, threadpoolctl, scikit-learn, tzdata, pytz, six, python-dateutil, pandas, tqdm, click, regex, nltk, nvidia-cufft-cu11, MarkupSafe, jinja2, networkx, nvidia-cuda-cupti-cu11, typing-extensions, mpmath, sympy, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-nccl-cu11, nvidia-curand-cu11, lit, filelock, cmake, triton, nvidia-cuda-runtime-cu11, nvidia-cusparse-cu11, nvidia-nvtx-cu11, nvidia-cudnn-cu11, nvidia-cuda-nvrtc-cu11, torch, urllib3, idna, certifi, charset-normalizer, requests, pillow, torchvision, grpcio, gast, google-pasta, opt-einsum, flatbuffers, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, keras, wrapt, protobuf, ml-dtypes, jax, absl-py, zipp, importlib-metadata, markdown, tensorboard-data-server, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, werkzeug, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, libclang, packaging, astunparse, h5py, tensorflow, pyyaml, huggingface-hub, sentencepiece, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 certifi-2022.12.7 charset-normalizer-3.1.0 click-8.1.3 cmake-3.26.1 filelock-3.10.7 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.53.0 h5py-3.8.0 huggingface-hub-0.13.3 idna-3.4 importlib-metadata-6.1.0 jax-0.4.8 jinja2-3.1.2 joblib-1.2.0 keras-2.12.0 libclang-16.0.0 lit-16.0.0 markdown-3.4.3 ml-dtypes-0.0.4 mpmath-1.3.0 networkx-3.0 nltk-3.8.1 numpy-1.22.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.0 pandas-2.0.0 pillow-9.5.0 protobuf-4.22.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.2 pytz-2023.3 pyyaml-6.0 regex-2023.3.23 requests-2.28.2 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.2.2 scipy-1.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 six-1.16.0 sympy-1.11.1 tensorboard-2.12.1 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.2.0 threadpoolctl-3.1.0 tokenizers-0.13.2 torch-2.0.0 torchvision-0.15.1 tqdm-4.65.0 transformers-4.27.4 triton-2.0.0 typing-extensions-4.5.0 tzdata-2023.3 urllib3-1.26.15 werkzeug-2.2.3 wrapt-1.14.1 zipp-3.15.0\n",
      "Removing intermediate container 5111adce08df\n",
      " ---> 194b5101966f\n",
      "Step 5/9 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in d2af553fdbbe\n",
      "Removing intermediate container d2af553fdbbe\n",
      " ---> 3e48d50703ce\n",
      "Step 6/9 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Running in 090f892418f4\n",
      "Removing intermediate container 090f892418f4\n",
      " ---> 23bf57fc777d\n",
      "Step 7/9 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Running in 143df9d177f7\n",
      "Removing intermediate container 143df9d177f7\n",
      " ---> 1494147d1d63\n",
      "Step 8/9 : COPY codes /opt/program\n",
      " ---> 27b676924092\n",
      "Step 9/9 : WORKDIR /opt/program\n",
      " ---> Running in 9ff5e4b52b51\n",
      "Removing intermediate container 9ff5e4b52b51\n",
      " ---> 474113c94185\n",
      "Successfully built 474113c94185\n",
      "Successfully tagged custom-sentence-transformer:latest\n",
      "The push refers to repository [639115409343.dkr.ecr.ap-southeast-1.amazonaws.com/custom-sentence-transformer]\n",
      "9a717a434c69: Preparing\n",
      "b66a4a9bd308: Preparing\n",
      "d333ff65f7d0: Preparing\n",
      "e6697b250859: Preparing\n",
      "62789ffcd78c: Preparing\n",
      "62789ffcd78c: Layer already exists\n",
      "d333ff65f7d0: Pushed\n",
      "9a717a434c69: Pushed\n",
      "e6697b250859: Pushed\n",
      "b66a4a9bd308: Pushed\n",
      "latest: digest: sha256:f486a187c7db8e82ec1284d4a11bcdefe303724d7285204b5cb7c2e79891d38c size: 1370\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=custom-sentence-transformer\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x codes/train\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-ap-southeast-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
